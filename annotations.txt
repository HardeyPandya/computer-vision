Viola-Jones Algorithm
    Detection: Convolution + haar-like features + averaging

Haar-like features:
    quadrados escal√°veis preto e branco
Integral Image:
    imagem gerada antes do processamento, na qual cada p√≠xel cont√©m a soma
    dos p√≠xeis √† esquerda e √† direita da imagem Integral
    Permite o c√°lculo da m√©dia muito mais r√°pido, atrav√©s de apenas 4 opera√ß√µes
    de soma e subtra√ß√£o

Training classifiers
    Decidir features, thresholds
    Shrink to 24X24
    +4000 faces in original paper
    +9000 random images without faces

adaboost
    Emsemble method
    Many weak classifiers are summed to form a strong classifiers
    Idea is combine features that complement  each other
    Not pick the best features, but to build the best classifiers

cascading
    take a big kernel (sub-image)
    look for the first (most important) feature
    if its not present, skip to next kernel
    else look for the next feature
    
Deep Sparse Rectifier Neural Networks
    Rectifier activation function is very good for sentiment analisis
    in bag of words, binary vectors, reviwe-stars approach.

Neural Networks
    y - actual value
    ^y - prediction

    Gradient descent, Batch
        Prediction for all the epoch; 
        Cost fuction is calculated for all the epoch, and the average is taken;
        C=sum(1/2(y^-y)¬≤)
        Weights are updated;
        Next Iteration;
        epoch - pass trhough the whole dataset
        -more chance of finding local minimum

    Stochastic Gradient descent
        Predictions for single observation
        Cost for single observation
        Updates Weights
        -faster and more fluctuation => more chance of finding global minimum

    Mini-batch gradient descent 
        Combine the two methods

Neuroevolu√ß√£o - IA, Denis
    ùëß = ‚àëùë§ùëñ ùë•ùëñ
    Em nota√ß√£o vetorial:
    ùëß = wùëá x
    Tamb√©m √© poss√≠vel substituir a somat√≥ria pelo produto interno
    w ‚ãÖ x

RNN - LSTM
    RNN s√£o √∫teis para lembrar de dados anteriores
    RNNs comuns t√™m problemas com dados muito antigos
    LSTM resolve esse problema
    RNN comuns eram compostas de m√∫ltiplas c√©lulas de uma camada de tanh
        Uma camada √© composta pelos neur√¥nios de entrada, seus pesos,
        seu bias, uma soma ponderada pelos pesos, e uma fun√ß√£o de ativa√ß√£o
        A fun√ß√£o de ativa√ß√£o Tanh gera um resultado pertencente ao d (-1,1)
        J√° a sigmoide, [0,1]
        Tanh √© uma fun√ß√£o de ativa√ß√£o
    Cell state
        Linha horizontal superior que flui em todas as c√©lulas
        Gate - camada sigmoid + pointwise multiplication na cell state
            Quanto mais pr√≥ximo de 1 for a sa√≠da da camada,
            maior a mudan√ßa no Cell state
    
    *Opera√ß√µes
        Produto escalar: u . v
            Entrada s√£o 2 vetores
            Sa√≠da √© um escalar
        Produto vetorial: u x v
            Entrada s√£o 2 vetores
            Sa√≠da √© um vetor
        Produto interno: AB
            Ordem importa
            Entrada s√£o 2 matrizes
            Sa√≠da √© uma matriz

    Step-by-step
        Forget gate layer
            Camada sigmoid + pointwise multiplication
            Decide quanto vai ser mantida da informa√ß√£o anterior
            Recebe ht‚àí1 e xt, onde xt √© o input
            Equa√ß√£o:
                ht-1 √© um vetor de outputs da c√©lula anterior
                xt √© o vetor de entrada
                Sa√≠da = sigmoide(matrizPesos produtoInterno concatena√ß√£o(ht-1, xt) + vetorBias)
        New information
            Input Gate layer
                Quais valores ser√£o atualizados
                Sa√≠da = sigmoide(pesos produtoInterno concatena√ß√£o(ht-1, xt) + bias)
            Tanh layer
                Cria vetor de novos candidatos ~C
                ~Ct = tanh(pesos produtoInterno concatena√ß√£o(ht-1, xt) + bias)
        
        Updating the cell state
            Estado antigo (Ct-1) √© multiplicado pela sa√≠da do Forget gate layer(ft)
            Adiciona-se (it * ~Ct), a sa√≠da da Tanh layer escalada pela sa√≠da do Input Gate layer
            Ct = (ft * Ct-1) + (it * ~Ct)
        
        Output
            Baseado no Cell State (e input?) por√©m filtrado
            Sigmoid Layer
                Decide qual partes do Cell State passam
            Tanh function
                Para alterar dom√≠nio do Output
            